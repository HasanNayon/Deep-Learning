{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasanNayon/Deep-Learning/blob/main/LSTM_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IrmA3eaj3kF0"
      },
      "outputs": [],
      "source": [
        "faqs = \"\"\"Artificial intelligence (AI) has rapidly evolved over the past few decades, revolutionizing various industries and significantly impacting human lives.\n",
        "One of the most exciting advancements in AI is natural language processing (NLP), which enables machines to understand, generate, and interact with human language.\n",
        "From virtual assistants like Siri and Alexa to sophisticated chatbots and language translation tools, NLP has made human-computer interaction more seamless than ever before.\n",
        "Deep learning, a powerful subset of machine learning, plays a crucial role in these advancements by allowing AI models to learn patterns from vast amounts of text data.\n",
        "Recurrent Neural Networks (RNNs) and Transformer models, such as GPT (Generative Pre-trained Transformer), have demonstrated remarkable performance in text prediction, machine translation, and sentiment analysis.\n",
        "These models rely on vast datasets and complex architectures to predict the next word in a sentence with high accuracy, making them valuable for applications such as auto-complete, spell checking, and AI-driven content creation.\n",
        "\n",
        "As AI-powered language models continue to improve, their applications expand into education, healthcare, customer service, and content generation.\n",
        "In education, AI-driven tutoring systems provide personalized learning experiences, adapting to students' strengths and weaknesses.\n",
        "In healthcare, AI assists doctors by analyzing medical records, predicting potential diagnoses, and even generating patient reports.\n",
        "Businesses leverage AI chatbots to enhance customer interactions, offering quick responses and support without human intervention.\n",
        "Despite these advancements, challenges remain, including ethical concerns, bias in AI models, and the potential misuse of AI-generated content.\n",
        "Researchers are working on developing fair, unbiased, and responsible AI systems that align with ethical standards.\n",
        "With continuous improvements in deep learning and computational power,\n",
        "the future of AI-driven text prediction holds immense potential, reshaping how humans interact with technology and unlocking new possibilities in artificial intelligence\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "J1D42emD32Ro"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "KhtDxwL_AXFj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "K8MRFre9AaG9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrpAl3EDAgvh",
        "outputId": "c957ed69-6930-47d4-d933-5744e8b22ab1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in faqs.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "44VahqKdAjr9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyqwPDzNA5mR",
        "outputId": "c29b1f4a-5f2f-4095-86de-5c95d97567cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 21],\n",
              " [20, 21, 2],\n",
              " [20, 21, 2, 22],\n",
              " [20, 21, 2, 22, 42],\n",
              " [20, 21, 2, 22, 42, 43],\n",
              " [20, 21, 2, 22, 42, 43, 44],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48, 49],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 1],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 1, 51],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 1, 51, 52],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 1, 51, 52, 9],\n",
              " [20, 21, 2, 22, 42, 43, 44, 5, 45, 46, 47, 48, 49, 50, 1, 51, 52, 9, 53],\n",
              " [54, 6],\n",
              " [54, 6, 5],\n",
              " [54, 6, 5, 55],\n",
              " [54, 6, 5, 55, 56],\n",
              " [54, 6, 5, 55, 56, 12],\n",
              " [54, 6, 5, 55, 56, 12, 3],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23, 60],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23, 60, 61],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23, 60, 61, 62],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23, 60, 61, 62, 4],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23, 60, 61, 62, 4, 63],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23, 60, 61, 62, 4, 63, 64],\n",
              " [54, 6, 5, 55, 56, 12, 3, 2, 57, 58, 10, 59, 23, 60, 61, 62, 4, 63, 64, 1],\n",
              " [54,\n",
              "  6,\n",
              "  5,\n",
              "  55,\n",
              "  56,\n",
              "  12,\n",
              "  3,\n",
              "  2,\n",
              "  57,\n",
              "  58,\n",
              "  10,\n",
              "  59,\n",
              "  23,\n",
              "  60,\n",
              "  61,\n",
              "  62,\n",
              "  4,\n",
              "  63,\n",
              "  64,\n",
              "  1,\n",
              "  24],\n",
              " [54,\n",
              "  6,\n",
              "  5,\n",
              "  55,\n",
              "  56,\n",
              "  12,\n",
              "  3,\n",
              "  2,\n",
              "  57,\n",
              "  58,\n",
              "  10,\n",
              "  59,\n",
              "  23,\n",
              "  60,\n",
              "  61,\n",
              "  62,\n",
              "  4,\n",
              "  63,\n",
              "  64,\n",
              "  1,\n",
              "  24,\n",
              "  7],\n",
              " [54,\n",
              "  6,\n",
              "  5,\n",
              "  55,\n",
              "  56,\n",
              "  12,\n",
              "  3,\n",
              "  2,\n",
              "  57,\n",
              "  58,\n",
              "  10,\n",
              "  59,\n",
              "  23,\n",
              "  60,\n",
              "  61,\n",
              "  62,\n",
              "  4,\n",
              "  63,\n",
              "  64,\n",
              "  1,\n",
              "  24,\n",
              "  7,\n",
              "  9],\n",
              " [54,\n",
              "  6,\n",
              "  5,\n",
              "  55,\n",
              "  56,\n",
              "  12,\n",
              "  3,\n",
              "  2,\n",
              "  57,\n",
              "  58,\n",
              "  10,\n",
              "  59,\n",
              "  23,\n",
              "  60,\n",
              "  61,\n",
              "  62,\n",
              "  4,\n",
              "  63,\n",
              "  64,\n",
              "  1,\n",
              "  24,\n",
              "  7,\n",
              "  9,\n",
              "  10],\n",
              " [25, 65],\n",
              " [25, 65, 66],\n",
              " [25, 65, 66, 67],\n",
              " [25, 65, 66, 67, 68],\n",
              " [25, 65, 66, 67, 68, 1],\n",
              " [25, 65, 66, 67, 68, 1, 69],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27, 71],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27, 71, 23],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27, 71, 23, 22],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27, 71, 23, 22, 72],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27, 71, 23, 22, 72, 9],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27, 71, 23, 22, 72, 9, 73],\n",
              " [25, 65, 66, 67, 68, 1, 69, 4, 70, 26, 1, 10, 27, 71, 23, 22, 72, 9, 73, 74],\n",
              " [25,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  1,\n",
              "  69,\n",
              "  4,\n",
              "  70,\n",
              "  26,\n",
              "  1,\n",
              "  10,\n",
              "  27,\n",
              "  71,\n",
              "  23,\n",
              "  22,\n",
              "  72,\n",
              "  9,\n",
              "  73,\n",
              "  74,\n",
              "  75],\n",
              " [25,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  1,\n",
              "  69,\n",
              "  4,\n",
              "  70,\n",
              "  26,\n",
              "  1,\n",
              "  10,\n",
              "  27,\n",
              "  71,\n",
              "  23,\n",
              "  22,\n",
              "  72,\n",
              "  9,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  76],\n",
              " [25,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  1,\n",
              "  69,\n",
              "  4,\n",
              "  70,\n",
              "  26,\n",
              "  1,\n",
              "  10,\n",
              "  27,\n",
              "  71,\n",
              "  23,\n",
              "  22,\n",
              "  72,\n",
              "  9,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  76,\n",
              "  77],\n",
              " [25,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  1,\n",
              "  69,\n",
              "  4,\n",
              "  70,\n",
              "  26,\n",
              "  1,\n",
              "  10,\n",
              "  27,\n",
              "  71,\n",
              "  23,\n",
              "  22,\n",
              "  72,\n",
              "  9,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  76,\n",
              "  77,\n",
              "  78],\n",
              " [25,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  1,\n",
              "  69,\n",
              "  4,\n",
              "  70,\n",
              "  26,\n",
              "  1,\n",
              "  10,\n",
              "  27,\n",
              "  71,\n",
              "  23,\n",
              "  22,\n",
              "  72,\n",
              "  9,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  76,\n",
              "  77,\n",
              "  78,\n",
              "  79],\n",
              " [28, 11],\n",
              " [28, 11, 13],\n",
              " [28, 11, 13, 80],\n",
              " [28, 11, 13, 80, 81],\n",
              " [28, 11, 13, 80, 81, 6],\n",
              " [28, 11, 13, 80, 81, 6, 29],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3, 14],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3, 14, 12],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3, 14, 12, 30],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3, 14, 12, 30, 85],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3, 14, 12, 30, 85, 2],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3, 14, 12, 30, 85, 2, 8],\n",
              " [28, 11, 13, 80, 81, 6, 29, 11, 82, 13, 83, 84, 3, 14, 12, 30, 85, 2, 8, 4],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86,\n",
              "  87],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86,\n",
              "  87,\n",
              "  25],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86,\n",
              "  87,\n",
              "  25,\n",
              "  31],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86,\n",
              "  87,\n",
              "  25,\n",
              "  31,\n",
              "  88],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86,\n",
              "  87,\n",
              "  25,\n",
              "  31,\n",
              "  88,\n",
              "  6],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86,\n",
              "  87,\n",
              "  25,\n",
              "  31,\n",
              "  88,\n",
              "  6,\n",
              "  15],\n",
              " [28,\n",
              "  11,\n",
              "  13,\n",
              "  80,\n",
              "  81,\n",
              "  6,\n",
              "  29,\n",
              "  11,\n",
              "  82,\n",
              "  13,\n",
              "  83,\n",
              "  84,\n",
              "  3,\n",
              "  14,\n",
              "  12,\n",
              "  30,\n",
              "  85,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  86,\n",
              "  87,\n",
              "  25,\n",
              "  31,\n",
              "  88,\n",
              "  6,\n",
              "  15,\n",
              "  89],\n",
              " [90, 91],\n",
              " [90, 91, 92],\n",
              " [90, 91, 92, 93],\n",
              " [90, 91, 92, 93, 1],\n",
              " [90, 91, 92, 93, 1, 32],\n",
              " [90, 91, 92, 93, 1, 32, 8],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96, 97],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96, 97, 32],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96, 97, 32, 98],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96, 97, 32, 98, 99],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96, 97, 32, 98, 99, 100],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96, 97, 32, 98, 99, 100, 101],\n",
              " [90, 91, 92, 93, 1, 32, 8, 33, 16, 94, 95, 96, 97, 32, 98, 99, 100, 101, 3],\n",
              " [90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  1,\n",
              "  32,\n",
              "  8,\n",
              "  33,\n",
              "  16,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  32,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  3,\n",
              "  15],\n",
              " [90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  1,\n",
              "  32,\n",
              "  8,\n",
              "  33,\n",
              "  16,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  32,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  3,\n",
              "  15,\n",
              "  34],\n",
              " [90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  1,\n",
              "  32,\n",
              "  8,\n",
              "  33,\n",
              "  16,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  32,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  3,\n",
              "  15,\n",
              "  34,\n",
              "  29],\n",
              " [90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  1,\n",
              "  32,\n",
              "  8,\n",
              "  33,\n",
              "  16,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  32,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  3,\n",
              "  15,\n",
              "  34,\n",
              "  29,\n",
              "  27],\n",
              " [90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  1,\n",
              "  32,\n",
              "  8,\n",
              "  33,\n",
              "  16,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  32,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  3,\n",
              "  15,\n",
              "  34,\n",
              "  29,\n",
              "  27,\n",
              "  1],\n",
              " [90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  1,\n",
              "  32,\n",
              "  8,\n",
              "  33,\n",
              "  16,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  32,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  3,\n",
              "  15,\n",
              "  34,\n",
              "  29,\n",
              "  27,\n",
              "  1,\n",
              "  102],\n",
              " [90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  1,\n",
              "  32,\n",
              "  8,\n",
              "  33,\n",
              "  16,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  32,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  3,\n",
              "  15,\n",
              "  34,\n",
              "  29,\n",
              "  27,\n",
              "  1,\n",
              "  102,\n",
              "  103],\n",
              " [14, 8],\n",
              " [14, 8, 104],\n",
              " [14, 8, 104, 35],\n",
              " [14, 8, 104, 35, 31],\n",
              " [14, 8, 104, 35, 31, 105],\n",
              " [14, 8, 104, 35, 31, 105, 1],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108, 5],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108, 5, 109],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108, 5, 109, 110],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108, 5, 109, 110, 3],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108, 5, 109, 110, 3, 13],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108, 5, 109, 110, 3, 13, 111],\n",
              " [14, 8, 104, 35, 31, 105, 1, 106, 107, 4, 108, 5, 109, 110, 3, 13, 111, 7],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119,\n",
              "  120],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119,\n",
              "  120,\n",
              "  121],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119,\n",
              "  120,\n",
              "  121,\n",
              "  1],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119,\n",
              "  120,\n",
              "  121,\n",
              "  1,\n",
              "  2],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119,\n",
              "  120,\n",
              "  121,\n",
              "  1,\n",
              "  2,\n",
              "  17],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119,\n",
              "  120,\n",
              "  121,\n",
              "  1,\n",
              "  2,\n",
              "  17,\n",
              "  18],\n",
              " [14,\n",
              "  8,\n",
              "  104,\n",
              "  35,\n",
              "  31,\n",
              "  105,\n",
              "  1,\n",
              "  106,\n",
              "  107,\n",
              "  4,\n",
              "  108,\n",
              "  5,\n",
              "  109,\n",
              "  110,\n",
              "  3,\n",
              "  13,\n",
              "  111,\n",
              "  7,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  36,\n",
              "  33,\n",
              "  16,\n",
              "  118,\n",
              "  119,\n",
              "  120,\n",
              "  121,\n",
              "  1,\n",
              "  2,\n",
              "  17,\n",
              "  18,\n",
              "  122],\n",
              " [16, 2],\n",
              " [16, 2, 123],\n",
              " [16, 2, 123, 10],\n",
              " [16, 2, 123, 10, 8],\n",
              " [16, 2, 123, 10, 8, 124],\n",
              " [16, 2, 123, 10, 8, 124, 4],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127, 128],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127, 128, 37],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127, 128, 37, 38],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127, 128, 37, 38, 39],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127, 128, 37, 38, 39, 129],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127, 128, 37, 38, 39, 129, 1],\n",
              " [16, 2, 123, 10, 8, 124, 4, 125, 126, 36, 127, 128, 37, 38, 39, 129, 1, 18],\n",
              " [16,\n",
              "  2,\n",
              "  123,\n",
              "  10,\n",
              "  8,\n",
              "  124,\n",
              "  4,\n",
              "  125,\n",
              "  126,\n",
              "  36,\n",
              "  127,\n",
              "  128,\n",
              "  37,\n",
              "  38,\n",
              "  39,\n",
              "  129,\n",
              "  1,\n",
              "  18,\n",
              "  130],\n",
              " [3, 37],\n",
              " [3, 37, 2],\n",
              " [3, 37, 2, 17],\n",
              " [3, 37, 2, 17, 131],\n",
              " [3, 37, 2, 17, 131, 40],\n",
              " [3, 37, 2, 17, 131, 40, 132],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11, 134],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11, 134, 135],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11, 134, 135, 4],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11, 134, 135, 4, 136],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11, 134, 135, 4, 136, 137],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11, 134, 135, 4, 136, 137, 1],\n",
              " [3, 37, 2, 17, 131, 40, 132, 133, 11, 134, 135, 4, 136, 137, 1, 138],\n",
              " [3, 38],\n",
              " [3, 38, 2],\n",
              " [3, 38, 2, 139],\n",
              " [3, 38, 2, 139, 140],\n",
              " [3, 38, 2, 139, 140, 30],\n",
              " [3, 38, 2, 139, 140, 30, 141],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144, 19],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144, 19, 145],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144, 19, 145, 1],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144, 19, 145, 1, 146],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144, 19, 145, 1, 146, 147],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144, 19, 145, 1, 146, 147, 148],\n",
              " [3, 38, 2, 139, 140, 30, 141, 142, 143, 144, 19, 145, 1, 146, 147, 148, 149],\n",
              " [150, 151],\n",
              " [150, 151, 2],\n",
              " [150, 151, 2, 26],\n",
              " [150, 151, 2, 26, 4],\n",
              " [150, 151, 2, 26, 4, 152],\n",
              " [150, 151, 2, 26, 4, 152, 39],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154, 155],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154, 155, 156],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154, 155, 156, 1],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154, 155, 156, 1, 157],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154, 155, 156, 1, 157, 158],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154, 155, 156, 1, 157, 158, 9],\n",
              " [150, 151, 2, 26, 4, 152, 39, 153, 154, 155, 156, 1, 157, 158, 9, 159],\n",
              " [160, 14],\n",
              " [160, 14, 12],\n",
              " [160, 14, 12, 161],\n",
              " [160, 14, 12, 161, 162],\n",
              " [160, 14, 12, 161, 162, 163],\n",
              " [160, 14, 12, 161, 162, 163, 41],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8, 1],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8, 1, 5],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8, 1, 5, 19],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8, 1, 5, 19, 166],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8, 1, 5, 19, 166, 6],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8, 1, 5, 19, 166, 6, 2],\n",
              " [160, 14, 12, 161, 162, 163, 41, 164, 165, 3, 2, 8, 1, 5, 19, 166, 6, 2, 167],\n",
              " [160,\n",
              "  14,\n",
              "  12,\n",
              "  161,\n",
              "  162,\n",
              "  163,\n",
              "  41,\n",
              "  164,\n",
              "  165,\n",
              "  3,\n",
              "  2,\n",
              "  8,\n",
              "  1,\n",
              "  5,\n",
              "  19,\n",
              "  166,\n",
              "  6,\n",
              "  2,\n",
              "  167,\n",
              "  18],\n",
              " [168, 169],\n",
              " [168, 169, 170],\n",
              " [168, 169, 170, 35],\n",
              " [168, 169, 170, 35, 171],\n",
              " [168, 169, 170, 35, 171, 172],\n",
              " [168, 169, 170, 35, 171, 172, 173],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174, 2],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174, 2, 40],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174, 2, 40, 175],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174, 2, 40, 175, 176],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174, 2, 40, 175, 176, 7],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174, 2, 40, 175, 176, 7, 41],\n",
              " [168, 169, 170, 35, 171, 172, 173, 1, 174, 2, 40, 175, 176, 7, 41, 177],\n",
              " [7, 178],\n",
              " [7, 178, 179],\n",
              " [7, 178, 179, 3],\n",
              " [7, 178, 179, 3, 28],\n",
              " [7, 178, 179, 3, 28, 11],\n",
              " [7, 178, 179, 3, 28, 11, 1],\n",
              " [7, 178, 179, 3, 28, 11, 1, 180],\n",
              " [7, 178, 179, 3, 28, 11, 1, 180, 181],\n",
              " [5, 182],\n",
              " [5, 182, 6],\n",
              " [5, 182, 6, 2],\n",
              " [5, 182, 6, 2, 17],\n",
              " [5, 182, 6, 2, 17, 15],\n",
              " [5, 182, 6, 2, 17, 15, 34],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185, 186],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185, 186, 187],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185, 186, 187, 24],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185, 186, 187, 24, 7],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185, 186, 187, 24, 7, 188],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185, 186, 187, 24, 7, 188, 1],\n",
              " [5, 182, 6, 2, 17, 15, 34, 183, 184, 19, 185, 186, 187, 24, 7, 188, 1, 189],\n",
              " [5,\n",
              "  182,\n",
              "  6,\n",
              "  2,\n",
              "  17,\n",
              "  15,\n",
              "  34,\n",
              "  183,\n",
              "  184,\n",
              "  19,\n",
              "  185,\n",
              "  186,\n",
              "  187,\n",
              "  24,\n",
              "  7,\n",
              "  188,\n",
              "  1,\n",
              "  189,\n",
              "  190],\n",
              " [5,\n",
              "  182,\n",
              "  6,\n",
              "  2,\n",
              "  17,\n",
              "  15,\n",
              "  34,\n",
              "  183,\n",
              "  184,\n",
              "  19,\n",
              "  185,\n",
              "  186,\n",
              "  187,\n",
              "  24,\n",
              "  7,\n",
              "  188,\n",
              "  1,\n",
              "  189,\n",
              "  190,\n",
              "  191],\n",
              " [5,\n",
              "  182,\n",
              "  6,\n",
              "  2,\n",
              "  17,\n",
              "  15,\n",
              "  34,\n",
              "  183,\n",
              "  184,\n",
              "  19,\n",
              "  185,\n",
              "  186,\n",
              "  187,\n",
              "  24,\n",
              "  7,\n",
              "  188,\n",
              "  1,\n",
              "  189,\n",
              "  190,\n",
              "  191,\n",
              "  3],\n",
              " [5,\n",
              "  182,\n",
              "  6,\n",
              "  2,\n",
              "  17,\n",
              "  15,\n",
              "  34,\n",
              "  183,\n",
              "  184,\n",
              "  19,\n",
              "  185,\n",
              "  186,\n",
              "  187,\n",
              "  24,\n",
              "  7,\n",
              "  188,\n",
              "  1,\n",
              "  189,\n",
              "  190,\n",
              "  191,\n",
              "  3,\n",
              "  20],\n",
              " [5,\n",
              "  182,\n",
              "  6,\n",
              "  2,\n",
              "  17,\n",
              "  15,\n",
              "  34,\n",
              "  183,\n",
              "  184,\n",
              "  19,\n",
              "  185,\n",
              "  186,\n",
              "  187,\n",
              "  24,\n",
              "  7,\n",
              "  188,\n",
              "  1,\n",
              "  189,\n",
              "  190,\n",
              "  191,\n",
              "  3,\n",
              "  20,\n",
              "  21]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "CrzbvUUQCXPU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5NQQHfjxucI",
        "outputId": "2e0db72e-6166-4346-f143-3adcbd066c14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ],
      "metadata": {
        "id": "9oPMoWBSD1_U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miRb-QZyIi7_",
        "outputId": "6f162d39-52ae-4eb2-ffc6-e5e9601e08ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  20,  21],\n",
              "       [  0,   0,   0, ...,  20,  21,   2],\n",
              "       [  0,   0,   0, ...,  21,   2,  22],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 190, 191,   3],\n",
              "       [  0,   0,   0, ..., 191,   3,  20],\n",
              "       [  0,   0,   0, ...,   3,  20,  21]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]"
      ],
      "metadata": {
        "id": "qVI0-UUrIsd3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "lXrYHTDFI3uE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z8Qvcq3yAQ8",
        "outputId": "7d14d276-e734-40f3-8af0-1d78d264b452"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   0,  20],\n",
              "       [  0,   0,   0, ...,   0,  20,  21],\n",
              "       [  0,   0,   0, ...,  20,  21,   2],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 189, 190, 191],\n",
              "       [  0,   0,   0, ..., 190, 191,   3],\n",
              "       [  0,   0,   0, ..., 191,   3,  20]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxjVuOswyANn",
        "outputId": "ee79f1db-0a88-4e0e-cfa8-5be4696a147a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 21,   2,  22,  42,  43,  44,   5,  45,  46,  47,  48,  49,  50,\n",
              "         1,  51,  52,   9,  53,   6,   5,  55,  56,  12,   3,   2,  57,\n",
              "        58,  10,  59,  23,  60,  61,  62,   4,  63,  64,   1,  24,   7,\n",
              "         9,  10,  65,  66,  67,  68,   1,  69,   4,  70,  26,   1,  10,\n",
              "        27,  71,  23,  22,  72,   9,  73,  74,  75,  76,  77,  78,  79,\n",
              "        11,  13,  80,  81,   6,  29,  11,  82,  13,  83,  84,   3,  14,\n",
              "        12,  30,  85,   2,   8,   4,  86,  87,  25,  31,  88,   6,  15,\n",
              "        89,  91,  92,  93,   1,  32,   8,  33,  16,  94,  95,  96,  97,\n",
              "        32,  98,  99, 100, 101,   3,  15,  34,  29,  27,   1, 102, 103,\n",
              "         8, 104,  35,  31, 105,   1, 106, 107,   4, 108,   5, 109, 110,\n",
              "         3,  13, 111,   7, 112, 113, 114, 115, 116, 117,  36,  33,  16,\n",
              "       118, 119, 120, 121,   1,   2,  17,  18, 122,   2, 123,  10,   8,\n",
              "       124,   4, 125, 126,  36, 127, 128,  37,  38,  39, 129,   1,  18,\n",
              "       130,  37,   2,  17, 131,  40, 132, 133,  11, 134, 135,   4, 136,\n",
              "       137,   1, 138,  38,   2, 139, 140,  30, 141, 142, 143, 144,  19,\n",
              "       145,   1, 146, 147, 148, 149, 151,   2,  26,   4, 152,  39, 153,\n",
              "       154, 155, 156,   1, 157, 158,   9, 159,  14,  12, 161, 162, 163,\n",
              "        41, 164, 165,   3,   2,   8,   1,   5,  19, 166,   6,   2, 167,\n",
              "        18, 169, 170,  35, 171, 172, 173,   1, 174,   2,  40, 175, 176,\n",
              "         7,  41, 177, 178, 179,   3,  28,  11,   1, 180, 181, 182,   6,\n",
              "         2,  17,  15,  34, 183, 184,  19, 185, 186, 187,  24,   7, 188,\n",
              "         1, 189, 190, 191,   3,  20,  21], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmsFnHx1Qdow",
        "outputId": "be0814d9-6caa-4bab-f838-708c99b6332e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(280, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wyYqYgZSeck",
        "outputId": "6b4c64e9-a64e-4ab8-b062-b3f386aad077"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(280,)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5OL3vrEXSs_s"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,num_classes=192)"
      ],
      "metadata": {
        "id": "rs1NPitwSgzk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQMJ0I6xSiZf",
        "outputId": "8242afba-20bc-41e5-f4e8-572f98a42492"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(280, 192)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM,Dense"
      ],
      "metadata": {
        "id": "elKcmEcPzdeV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()\n",
        "model.add(Embedding(192,100,input_length=56))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(192,activation='softmax'))"
      ],
      "metadata": {
        "id": "fDhOVw_2zdbb",
        "outputId": "defed2d5-3a1f-4079-85f3-b88eee5bbf9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "6tZcx0PrzdZC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "19VWw-m2zdWG",
        "outputId": "692456f4-5be7-4e94-fecf-43a5db6b3a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " embedding (\u001b[38;5;33mEmbedding\u001b[0m)                 ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " lstm (\u001b[38;5;33mLSTM\u001b[0m)                           ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)                         ?                                \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                         </span><span style=\"font-weight: bold\"> Output Shape                </span><span style=\"font-weight: bold\">         Param # </span>\n",
              "\n",
              " embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                 ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                           ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                         ?                                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpFUCALCfJRR",
        "outputId": "14c7a080-876a-4468-b1cc-95145c436834"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.1635\n",
            "Epoch 2/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9979 - loss: 0.1525\n",
            "Epoch 3/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9924 - loss: 0.1495\n",
            "Epoch 4/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.1430\n",
            "Epoch 5/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.1479\n",
            "Epoch 6/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.1401\n",
            "Epoch 7/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.1341\n",
            "Epoch 8/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.1303\n",
            "Epoch 9/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.1314\n",
            "Epoch 10/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9962 - loss: 0.1241\n",
            "Epoch 11/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.1214\n",
            "Epoch 12/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9912 - loss: 0.1197\n",
            "Epoch 13/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.1151\n",
            "Epoch 14/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.1175\n",
            "Epoch 15/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.1104\n",
            "Epoch 16/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.1053\n",
            "Epoch 17/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.1073\n",
            "Epoch 18/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9881 - loss: 0.1102\n",
            "Epoch 19/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.1026\n",
            "Epoch 20/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9972 - loss: 0.0978\n",
            "Epoch 21/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0970\n",
            "Epoch 22/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0924\n",
            "Epoch 23/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9918 - loss: 0.0939\n",
            "Epoch 24/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9873 - loss: 0.0954\n",
            "Epoch 25/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9952 - loss: 0.0873\n",
            "Epoch 26/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9965 - loss: 0.0914\n",
            "Epoch 27/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9984 - loss: 0.0859\n",
            "Epoch 28/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9989 - loss: 0.0813\n",
            "Epoch 29/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9887 - loss: 0.0866\n",
            "Epoch 30/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9934 - loss: 0.0789\n",
            "Epoch 31/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9958 - loss: 0.0762\n",
            "Epoch 32/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9965 - loss: 0.0782\n",
            "Epoch 33/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9965 - loss: 0.0740\n",
            "Epoch 34/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9989 - loss: 0.0721\n",
            "Epoch 35/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9993 - loss: 0.0783\n",
            "Epoch 36/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9973 - loss: 0.0714\n",
            "Epoch 37/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9984 - loss: 0.0764\n",
            "Epoch 38/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9955 - loss: 0.0708\n",
            "Epoch 39/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0654\n",
            "Epoch 40/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0653\n",
            "Epoch 41/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9984 - loss: 0.0638\n",
            "Epoch 42/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9973 - loss: 0.0606\n",
            "Epoch 43/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0650\n",
            "Epoch 44/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9918 - loss: 0.0645\n",
            "Epoch 45/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0615\n",
            "Epoch 46/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9881 - loss: 0.0655\n",
            "Epoch 47/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9989 - loss: 0.0569\n",
            "Epoch 48/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9887 - loss: 0.0618\n",
            "Epoch 49/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9897 - loss: 0.0609\n",
            "Epoch 50/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0558\n",
            "Epoch 51/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9948 - loss: 0.0538\n",
            "Epoch 52/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0541\n",
            "Epoch 53/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0507\n",
            "Epoch 54/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0537\n",
            "Epoch 55/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0532\n",
            "Epoch 56/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9924 - loss: 0.0535\n",
            "Epoch 57/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0482\n",
            "Epoch 58/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9892 - loss: 0.0545\n",
            "Epoch 59/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0503\n",
            "Epoch 60/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.0495\n",
            "Epoch 61/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9901 - loss: 0.0498\n",
            "Epoch 62/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0471\n",
            "Epoch 63/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0511\n",
            "Epoch 64/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9908 - loss: 0.0519\n",
            "Epoch 65/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0440\n",
            "Epoch 66/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0422\n",
            "Epoch 67/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9944 - loss: 0.0433\n",
            "Epoch 68/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9958 - loss: 0.0413\n",
            "Epoch 69/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0436\n",
            "Epoch 70/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.0450\n",
            "Epoch 71/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0401\n",
            "Epoch 72/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0391\n",
            "Epoch 73/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.0422\n",
            "Epoch 74/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9948 - loss: 0.0390\n",
            "Epoch 75/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9863 - loss: 0.0465\n",
            "Epoch 76/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0393\n",
            "Epoch 77/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0421\n",
            "Epoch 78/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9962 - loss: 0.0362\n",
            "Epoch 79/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.0398\n",
            "Epoch 80/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9924 - loss: 0.0384\n",
            "Epoch 81/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9939 - loss: 0.0406\n",
            "Epoch 82/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0352\n",
            "Epoch 83/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9887 - loss: 0.0401\n",
            "Epoch 84/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9939 - loss: 0.0414\n",
            "Epoch 85/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9952 - loss: 0.0336\n",
            "Epoch 86/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0369\n",
            "Epoch 87/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0307\n",
            "Epoch 88/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0313\n",
            "Epoch 89/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9924 - loss: 0.0351\n",
            "Epoch 90/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0310\n",
            "Epoch 91/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0305\n",
            "Epoch 92/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9948 - loss: 0.0317\n",
            "Epoch 93/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0296\n",
            "Epoch 94/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9928 - loss: 0.0324\n",
            "Epoch 95/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0291\n",
            "Epoch 96/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9924 - loss: 0.0330\n",
            "Epoch 97/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9908 - loss: 0.0386\n",
            "Epoch 98/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0296\n",
            "Epoch 99/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0277\n",
            "Epoch 100/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0342\n",
            "Epoch 101/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0281\n",
            "Epoch 102/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0261\n",
            "Epoch 103/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9908 - loss: 0.0360\n",
            "Epoch 104/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0264\n",
            "Epoch 105/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9881 - loss: 0.0322\n",
            "Epoch 106/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.0288\n",
            "Epoch 107/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9918 - loss: 0.0291\n",
            "Epoch 108/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0251\n",
            "Epoch 109/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9944 - loss: 0.0264\n",
            "Epoch 110/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0271\n",
            "Epoch 111/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0256\n",
            "Epoch 112/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0247\n",
            "Epoch 113/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0243\n",
            "Epoch 114/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0263\n",
            "Epoch 115/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0262\n",
            "Epoch 116/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0224\n",
            "Epoch 117/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0226\n",
            "Epoch 118/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.0237\n",
            "Epoch 119/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0217\n",
            "Epoch 120/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9928 - loss: 0.0256\n",
            "Epoch 121/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0253\n",
            "Epoch 122/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0283\n",
            "Epoch 123/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0238\n",
            "Epoch 124/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0212\n",
            "Epoch 125/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0235\n",
            "Epoch 126/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0228\n",
            "Epoch 127/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9979 - loss: 0.0210\n",
            "Epoch 128/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0214\n",
            "Epoch 129/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9973 - loss: 0.0250\n",
            "Epoch 130/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9973 - loss: 0.0206\n",
            "Epoch 131/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9984 - loss: 0.0209\n",
            "Epoch 132/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9973 - loss: 0.0230\n",
            "Epoch 133/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9989 - loss: 0.0235\n",
            "Epoch 134/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9993 - loss: 0.0186\n",
            "Epoch 135/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9993 - loss: 0.0204\n",
            "Epoch 136/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9993 - loss: 0.0172\n",
            "Epoch 137/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9928 - loss: 0.0224\n",
            "Epoch 138/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0175\n",
            "Epoch 139/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9984 - loss: 0.0195\n",
            "Epoch 140/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9965 - loss: 0.0241\n",
            "Epoch 141/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0202\n",
            "Epoch 142/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9939 - loss: 0.0240\n",
            "Epoch 143/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0216\n",
            "Epoch 144/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9954 - loss: 0.0187\n",
            "Epoch 145/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0176\n",
            "Epoch 146/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0186\n",
            "Epoch 147/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9954 - loss: 0.0181\n",
            "Epoch 148/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0207\n",
            "Epoch 149/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0187\n",
            "Epoch 150/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9965 - loss: 0.0200\n",
            "Epoch 151/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9881 - loss: 0.0233\n",
            "Epoch 152/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0157\n",
            "Epoch 153/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9928 - loss: 0.0196\n",
            "Epoch 154/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9918 - loss: 0.0200\n",
            "Epoch 155/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0161\n",
            "Epoch 156/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9924 - loss: 0.0191\n",
            "Epoch 157/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0152\n",
            "Epoch 158/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9939 - loss: 0.0242\n",
            "Epoch 159/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0147\n",
            "Epoch 160/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0190\n",
            "Epoch 161/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9979 - loss: 0.0176\n",
            "Epoch 162/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9881 - loss: 0.0221\n",
            "Epoch 163/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0226\n",
            "Epoch 164/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.0168\n",
            "Epoch 165/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0163\n",
            "Epoch 166/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0161\n",
            "Epoch 167/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9873 - loss: 0.0216\n",
            "Epoch 168/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.0149\n",
            "Epoch 169/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0211\n",
            "Epoch 170/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0164\n",
            "Epoch 171/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0191\n",
            "Epoch 172/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0159\n",
            "Epoch 173/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9908 - loss: 0.0248\n",
            "Epoch 174/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0145\n",
            "Epoch 175/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9944 - loss: 0.0157\n",
            "Epoch 176/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.0169\n",
            "Epoch 177/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.0188\n",
            "Epoch 178/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0127\n",
            "Epoch 179/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0153\n",
            "Epoch 180/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0123\n",
            "Epoch 181/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0163\n",
            "Epoch 182/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9939 - loss: 0.0192\n",
            "Epoch 183/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0160\n",
            "Epoch 184/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0121\n",
            "Epoch 185/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0175\n",
            "Epoch 186/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.0190\n",
            "Epoch 187/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0178\n",
            "Epoch 188/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0166\n",
            "Epoch 189/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0168\n",
            "Epoch 190/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0124\n",
            "Epoch 191/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9958 - loss: 0.0127\n",
            "Epoch 192/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9908 - loss: 0.0183\n",
            "Epoch 193/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - loss: 0.0129\n",
            "Epoch 194/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0124\n",
            "Epoch 195/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - loss: 0.0147\n",
            "Epoch 196/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0161\n",
            "Epoch 197/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - loss: 0.0151\n",
            "Epoch 198/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9984 - loss: 0.0119\n",
            "Epoch 199/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0189\n",
            "Epoch 200/200\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9979 - loss: 0.0120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bd371096750>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "text = \"Machine learning is\"\n",
        "\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=56, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGeYGwCMfTus",
        "outputId": "e88cf03f-d1fe-40dc-fa86-9758e5acd745"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "Machine learning is powerful\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Machine learning is powerful subset\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Machine learning is powerful subset of\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Machine learning is powerful subset of machine\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Machine learning is powerful subset of machine learning\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Machine learning is powerful subset of machine learning plays\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Machine learning is powerful subset of machine learning plays a\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Machine learning is powerful subset of machine learning plays a crucial\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Machine learning is powerful subset of machine learning plays a crucial role\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Machine learning is powerful subset of machine learning plays a crucial role in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTxsj-_CjbQW",
        "outputId": "4cc576eb-6ff7-4d4d-bb85-5913dfac5f44"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 1,\n",
              " 'ai': 2,\n",
              " 'in': 3,\n",
              " 'to': 4,\n",
              " 'the': 5,\n",
              " 'of': 6,\n",
              " 'with': 7,\n",
              " 'models': 8,\n",
              " 'human': 9,\n",
              " 'language': 10,\n",
              " 'learning': 11,\n",
              " 'advancements': 12,\n",
              " 'a': 13,\n",
              " 'these': 14,\n",
              " 'text': 15,\n",
              " 'as': 16,\n",
              " 'driven': 17,\n",
              " 'content': 18,\n",
              " 'potential': 19,\n",
              " 'artificial': 20,\n",
              " 'intelligence': 21,\n",
              " 'has': 22,\n",
              " 'nlp': 23,\n",
              " 'interact': 24,\n",
              " 'from': 25,\n",
              " 'chatbots': 26,\n",
              " 'translation': 27,\n",
              " 'deep': 28,\n",
              " 'machine': 29,\n",
              " 'by': 30,\n",
              " 'vast': 31,\n",
              " 'transformer': 32,\n",
              " 'such': 33,\n",
              " 'prediction': 34,\n",
              " 'on': 35,\n",
              " 'applications': 36,\n",
              " 'education': 37,\n",
              " 'healthcare': 38,\n",
              " 'customer': 39,\n",
              " 'systems': 40,\n",
              " 'ethical': 41,\n",
              " 'rapidly': 42,\n",
              " 'evolved': 43,\n",
              " 'over': 44,\n",
              " 'past': 45,\n",
              " 'few': 46,\n",
              " 'decades': 47,\n",
              " 'revolutionizing': 48,\n",
              " 'various': 49,\n",
              " 'industries': 50,\n",
              " 'significantly': 51,\n",
              " 'impacting': 52,\n",
              " 'lives': 53,\n",
              " 'one': 54,\n",
              " 'most': 55,\n",
              " 'exciting': 56,\n",
              " 'is': 57,\n",
              " 'natural': 58,\n",
              " 'processing': 59,\n",
              " 'which': 60,\n",
              " 'enables': 61,\n",
              " 'machines': 62,\n",
              " 'understand': 63,\n",
              " 'generate': 64,\n",
              " 'virtual': 65,\n",
              " 'assistants': 66,\n",
              " 'like': 67,\n",
              " 'siri': 68,\n",
              " 'alexa': 69,\n",
              " 'sophisticated': 70,\n",
              " 'tools': 71,\n",
              " 'made': 72,\n",
              " 'computer': 73,\n",
              " 'interaction': 74,\n",
              " 'more': 75,\n",
              " 'seamless': 76,\n",
              " 'than': 77,\n",
              " 'ever': 78,\n",
              " 'before': 79,\n",
              " 'powerful': 80,\n",
              " 'subset': 81,\n",
              " 'plays': 82,\n",
              " 'crucial': 83,\n",
              " 'role': 84,\n",
              " 'allowing': 85,\n",
              " 'learn': 86,\n",
              " 'patterns': 87,\n",
              " 'amounts': 88,\n",
              " 'data': 89,\n",
              " 'recurrent': 90,\n",
              " 'neural': 91,\n",
              " 'networks': 92,\n",
              " 'rnns': 93,\n",
              " 'gpt': 94,\n",
              " 'generative': 95,\n",
              " 'pre': 96,\n",
              " 'trained': 97,\n",
              " 'have': 98,\n",
              " 'demonstrated': 99,\n",
              " 'remarkable': 100,\n",
              " 'performance': 101,\n",
              " 'sentiment': 102,\n",
              " 'analysis': 103,\n",
              " 'rely': 104,\n",
              " 'datasets': 105,\n",
              " 'complex': 106,\n",
              " 'architectures': 107,\n",
              " 'predict': 108,\n",
              " 'next': 109,\n",
              " 'word': 110,\n",
              " 'sentence': 111,\n",
              " 'high': 112,\n",
              " 'accuracy': 113,\n",
              " 'making': 114,\n",
              " 'them': 115,\n",
              " 'valuable': 116,\n",
              " 'for': 117,\n",
              " 'auto': 118,\n",
              " 'complete': 119,\n",
              " 'spell': 120,\n",
              " 'checking': 121,\n",
              " 'creation': 122,\n",
              " 'powered': 123,\n",
              " 'continue': 124,\n",
              " 'improve': 125,\n",
              " 'their': 126,\n",
              " 'expand': 127,\n",
              " 'into': 128,\n",
              " 'service': 129,\n",
              " 'generation': 130,\n",
              " 'tutoring': 131,\n",
              " 'provide': 132,\n",
              " 'personalized': 133,\n",
              " 'experiences': 134,\n",
              " 'adapting': 135,\n",
              " \"students'\": 136,\n",
              " 'strengths': 137,\n",
              " 'weaknesses': 138,\n",
              " 'assists': 139,\n",
              " 'doctors': 140,\n",
              " 'analyzing': 141,\n",
              " 'medical': 142,\n",
              " 'records': 143,\n",
              " 'predicting': 144,\n",
              " 'diagnoses': 145,\n",
              " 'even': 146,\n",
              " 'generating': 147,\n",
              " 'patient': 148,\n",
              " 'reports': 149,\n",
              " 'businesses': 150,\n",
              " 'leverage': 151,\n",
              " 'enhance': 152,\n",
              " 'interactions': 153,\n",
              " 'offering': 154,\n",
              " 'quick': 155,\n",
              " 'responses': 156,\n",
              " 'support': 157,\n",
              " 'without': 158,\n",
              " 'intervention': 159,\n",
              " 'despite': 160,\n",
              " 'challenges': 161,\n",
              " 'remain': 162,\n",
              " 'including': 163,\n",
              " 'concerns': 164,\n",
              " 'bias': 165,\n",
              " 'misuse': 166,\n",
              " 'generated': 167,\n",
              " 'researchers': 168,\n",
              " 'are': 169,\n",
              " 'working': 170,\n",
              " 'developing': 171,\n",
              " 'fair': 172,\n",
              " 'unbiased': 173,\n",
              " 'responsible': 174,\n",
              " 'that': 175,\n",
              " 'align': 176,\n",
              " 'standards': 177,\n",
              " 'continuous': 178,\n",
              " 'improvements': 179,\n",
              " 'computational': 180,\n",
              " 'power': 181,\n",
              " 'future': 182,\n",
              " 'holds': 183,\n",
              " 'immense': 184,\n",
              " 'reshaping': 185,\n",
              " 'how': 186,\n",
              " 'humans': 187,\n",
              " 'technology': 188,\n",
              " 'unlocking': 189,\n",
              " 'new': 190,\n",
              " 'possibilities': 191}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92y7gE6pj9EZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}